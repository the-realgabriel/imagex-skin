{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67fe60a2",
   "metadata": {},
   "source": [
    "# Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41de8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d5b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth = True\n",
    "except Exception as ex:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad08715",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /kaggle/working/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4561090d",
   "metadata": {},
   "source": [
    "# Reading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0918b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_train = pathlib.Path(\"../input/skin-cancer9-classesisic/Skin cancer ISIC The International Skin Imaging Collaboration/Train/\")\n",
    "data_dir_test = pathlib.Path(\"../input/skin-cancer9-classesisic/Skin cancer ISIC The International Skin Imaging Collaboration/Test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\n",
    "print(image_count_train)\n",
    "image_count_test = len(list(data_dir_test.glob('*/*.jpg')))\n",
    "print(image_count_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747c0b1",
   "metadata": {},
   "source": [
    "# Load the DataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "rnd_seed = 123\n",
    "random.seed(rnd_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c9b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir_train,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf121d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir_train,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd0a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir_test,\n",
    "  validation_split=0.9,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ed45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5809df",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(num_classes):\n",
    "  plt.subplot(3,3,i+1)\n",
    "  image = img.imread(str(list(data_dir_train.glob(class_names[i]+'/*.jpg'))[1]))\n",
    "  plt.title(class_names[i])\n",
    "  plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d45085",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds.take(1):\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7aa2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0a2939",
   "metadata": {},
   "source": [
    "# Model 1 : standard Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25264ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 9\n",
    "model = Sequential([layers.experimental.preprocessing.Rescaling \\\n",
    "                    (1.0/255,input_shape=(img_height,img_width,3))])\n",
    "\n",
    "model.add(Conv2D(32, 3,padding=\"same\",activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(64, 3,padding=\"same\",activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(128, 3,padding=\"same\",activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(256, 3,padding=\"same\",activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(512, 3,padding=\"same\",activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation=\"relu\"))\n",
    "model.add(Dense(units=num_classes, activation= 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335cd092",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer= opt,\n",
    "              loss= SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081283bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ce8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe6c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790958f6",
   "metadata": {},
   "source": [
    "## Findings :\n",
    "*   The model is overfitting because we can see the difference in accuracy in training data & accuracy in the validation data that is almost 20%.\n",
    "\n",
    "*   The training accuracy is just around 70-75% with 25 epochos and the model is yet to learn the many features.\n",
    "\n",
    "*  data imbalance might be causing the bais to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be40de42",
   "metadata": {},
   "source": [
    "# Model 2 : Data Augumentation with drop out layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c8a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\",\n",
    "                                                 input_shape=(img_height,\n",
    "                                                              img_width,\n",
    "                                                              3)),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "  ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbdf936",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    augmented_images = data_augmentation(images)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d5f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 9\n",
    "model = Sequential(data_augmentation)\n",
    "model.add(layers.experimental.preprocessing.Rescaling(1.0/255,input_shape=(img_height,img_width,3)))\n",
    "\n",
    "model.add(Conv2D(32, 3,padding=\"same\",activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(64, 3,padding=\"same\",activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(128, 3,padding=\"same\",activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Conv2D(256, 3,padding=\"same\",activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(512, 3,padding=\"same\",activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation=\"relu\"))\n",
    "model.add(Dense(units=num_classes, activation= 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c1011",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt,\n",
    "              loss= SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d8d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1cd0ed",
   "metadata": {},
   "source": [
    "## Findings\n",
    "* With data agumenatation and drop layer, the overfitting of the model is adressed to great extend. Earlier the train and validation accuracy difference was nearly 20%, with latest approach it's reduced to 2-3%.\n",
    "\n",
    "* The accuracy of the model is compromised heavily and decreased by fair bit from previous venilla model.\n",
    "\n",
    "* Considering above 2 points, there is still a scope of lot of improvement of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5c117",
   "metadata": {},
   "source": [
    "# Analysing the class imbalance of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "total = 0\n",
    "all_count = []\n",
    "class_name = []\n",
    "for i in range(num_classes):\n",
    "  count = len(list(data_dir_train.glob(class_names[i]+'/*.jpg')))\n",
    "  total += count\n",
    "print(\"total training image count = {} \\n\".format(total))\n",
    "print(\"-------------------------------------\")\n",
    "for i in range(num_classes):\n",
    "  count = len(list(data_dir_train.glob(class_names[i]+'/*.jpg')))\n",
    "  print(\"Class name = \",class_names[i])\n",
    "  print(\"count      = \",count)\n",
    "  print(\"proportion = \",count/total)\n",
    "  print(\"-------------------------------------\")\n",
    "  all_count.append(count)\n",
    "  class_name.append(class_names[i])\n",
    "\n",
    "temp_df = pd.DataFrame(list(zip(all_count, class_name)), columns = ['count', 'class_name'])\n",
    "sns.barplot(data=temp_df, y=\"count\", x=\"class_name\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9c32c",
   "metadata": {},
   "source": [
    "## Findings\n",
    "Data is hevily imbalance and hence due to that results and predictions will be baised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d783e23",
   "metadata": {},
   "source": [
    "# Augmentor : Class balance\n",
    "\n",
    "Using Augmentor (https://augmentor.readthedocs.io/en/master/) to create the equal distribution of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa13afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Augmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_training_dataset = '../input/skin-cancer9-classesisic/Skin cancer ISIC The International Skin Imaging Collaboration/Train/'\n",
    "import Augmentor\n",
    "for i in class_names:\n",
    "    p = Augmentor.Pipeline(path_to_training_dataset + i, output_directory='/kaggle/working/data/'+i+'/output/')\n",
    "    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
    "    p.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = pathlib.Path('/kaggle/working/data/')\n",
    "image_count_train = len(list(output_dir.glob('*/output/*.jpg')))\n",
    "print(image_count_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c60f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "total = 0\n",
    "all_count = []\n",
    "class_name = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "  count = len(list(output_dir.glob(class_names[i]+'/output/*.jpg')))\n",
    "  total += count\n",
    "print(\"total training image count = {} \\n\".format(total))\n",
    "print(\"-------------------------------------\")\n",
    "for i in range(num_classes):\n",
    "  count = len(list(output_dir.glob(class_names[i]+'/output/*.jpg')))\n",
    "  print(\"Class name = \",class_names[i])\n",
    "  print(\"count      = \",count)\n",
    "  print(\"proportion = \",count/total)\n",
    "  print(\"-------------------------------------\")\n",
    "  all_count.append(count)\n",
    "  class_name.append(class_names[i])\n",
    "\n",
    "\n",
    "temp_df = pd.DataFrame(list(zip(all_count, class_name)), columns = ['count', 'class_name'])\n",
    "sns.barplot(data=temp_df, y=\"count\", x=\"class_name\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba3f8b7",
   "metadata": {},
   "source": [
    "# Model 3 : Model with Class balance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3529636",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  output_dir,\n",
    "  seed=123,\n",
    "  validation_split = 0.2,\n",
    "  subset = 'training',\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  output_dir,\n",
    "  seed=123,\n",
    "  validation_split = 0.2,\n",
    "  subset = 'validation',\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c74c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 9\n",
    "model = Sequential([layers.experimental.preprocessing.Rescaling(1.0/255,input_shape=(img_height,img_width,3))])\n",
    "\n",
    "model.add(Conv2D(32, 3,padding=\"same\",activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(64, 3,padding=\"same\",activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(128, 3,padding=\"same\",activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Conv2D(256, 3,padding=\"same\",activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(512, 3,padding=\"same\",activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation=\"relu\"))\n",
    "model.add(Dense(units=num_classes, activation= 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86171a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer= opt,\n",
    "              loss = SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6861d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde2ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d54a4a",
   "metadata": {},
   "source": [
    "## Findings :\n",
    "* After rebalance/resampling of the data (that gave equal proportion of data )and raised the accuray of the mdoel to 90%. This addressed the low accurty problem.\n",
    "\n",
    "*  overfitting probelm is adressed and now difference between train and val set is nearly 4-5% diff.\n",
    "\n",
    "* with these results it's conclusive that current module with rebalanced data is the best module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca5b98",
   "metadata": {},
   "source": [
    "# Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6abb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a file to save models\n",
    "top_model_weights_path = '/kaggle/working/cnn_fc_model.h5'\n",
    "model.save_weights(top_model_weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf5537",
   "metadata": {},
   "outputs": [],
   "source": [
    "(eval_loss, eval_accuracy) = model.evaluate(test_ds, batch_size=batch_size, \\\n",
    "                                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc9b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))\n",
    "print(\"[INFO] Loss: {}\".format(eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae3b738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
